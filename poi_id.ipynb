{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Person of Interest Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "*Question 1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.* \n",
    "The project tries to predict the people involved in the Enron scheme, ‘persons of interest’. Machine learning uses information about financial benefits and email communications, trying to figuring out patterns that distinguished the people of interests versus the rest. The dataset given includes individual’s information on financial benefits (payments and stock) involved with the company and email communications. It is obvious that the financial information will be useful for identifying people involved since money is the motive of the scheme. Individuals with especially high rate of communication the people of interests are likely to be one.\n",
    "\n",
    "This is not a 50-50 bag of labels. There are 18 people of interests out of 144. There is a clear outlier with extreme financial data and this turns out to be the ‘total’ of all the data points, which is not a valid data point itself so I removed it. As for individuals that have extremely large financial values, I still keep the data points because they are people of interest. In terms of features, all features have missing data and some have a lot of missing data. **Therefore, I edited the feature_format.py file so that it properly scales the features using MinMaxScaler and adds an argument replace_median (replace NAs with median values).**\n",
    "\n",
    "**Note:**\n",
    "- Since this is a small dataset, I decided to run many grid-searched model. However, if you want to know the end of the movie, I find the model that applies principle component and logistic regression works best.\n",
    "- This is a project for my Data Science course at Udacity, and the code for feature_format.py(except for a few edits by me) and tester.py is written by them \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 pois out of 146 samples.\n",
      "salary                        51\n",
      "to_messages                   60\n",
      "deferral_payments            107\n",
      "total_payments                21\n",
      "exercised_stock_options       44\n",
      "bonus                         64\n",
      "restricted_stock              36\n",
      "shared_receipt_with_poi       60\n",
      "restricted_stock_deferred    128\n",
      "total_stock_value             20\n",
      "expenses                      51\n",
      "loan_advances                142\n",
      "from_messages                 60\n",
      "other                         53\n",
      "from_this_person_to_poi       60\n",
      "poi                            0\n",
      "director_fees                129\n",
      "deferred_income               97\n",
      "long_term_incentive           80\n",
      "from_poi_to_this_person       60\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Setting up the environment\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### Task 1: Select what features you'll use.\n",
    "### The first feature must be \"poi\".\n",
    "\n",
    " \n",
    "### Load the dictionary containing the / m. \n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    data = pd.DataFrame.from_dict(data_dict,orient='index', dtype=np.float).drop('email_address',1)\n",
    "\n",
    "# Remove irrelevant outliers\n",
    "#data = data[~(data.index == 'TOTAL')]\n",
    "print 'There are %i pois out of %i samples.' %(sum(data['poi']==1),len(data['poi']))\n",
    "\n",
    "# Summary of missing values across features:\n",
    "print data.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "* Question 2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? *\n",
    "I ended up keeping the features: \"shared_receipt_with_poi\",\"exercised_stock_options\", and \"from_frac\". My strategy of choosing predictors is to run ANOVA F-value for each feature in 1500 times stratified resampling (to take care of possible overfitting) and choosing the ones with the best scores from each category: payment, stock value, and email communication. I narrowed down to some set of features(see below), and the set of these 3 features yields the highest performance. \n",
    "\n",
    "I employed MinMaxScaler for SVM algorithm since the financial features have much larger values than email features, we need to scale them otherwise the effect of financial features would be exaggerated in SVM algorithm. Besides, I also edited the feature_format.py file so that it properly scales the features and adds an argument replace_median (replace NAs with median values).\n",
    "\n",
    "I added 3 new variables:\n",
    "-\t‘from_frac’, ‘to_frac’: proportion of to/ from emails of a person sent from/to a poi. The motivation is that pois may communicate with each other much more frequently. The idea is also stemmed from one of the lesson videos of Udacity. As you will see below, 'from_frac' has the highest performance and turns out to be one of the key features; removing it drives the recall down though slightly increases precision. 'to_frac' is in the feature group that has better scores. However, adding 'to_frac' has near zero effect on recall and drives down the precision.\n",
    "-\t‘total_money’: I added this variable because in case the pois try to hide their financial benefits by diversifying the way they receive the money, then this sum may tell information that the individuals  features don’t. This feature, however, has a middle-range performance score. Adding this feature decreases the model's performance in terms of both precion and recall. \n",
    "Please a table below for the effect of these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create new features: ‘from_frac’, ‘to_frac', and 'total_money':\n",
    "\n",
    "data = data.fillna(0)\n",
    "\n",
    "data['to_frac'] = data[\"from_poi_to_this_person\"]/data[\"to_messages\"]\n",
    "data['from_frac'] = data[\"from_this_person_to_poi\"]/data[\"from_messages\"]\n",
    "data['total_money'] = data['total_payments'] + data[\"total_stock_value\"]\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Convert the dataset to dictionary for later submission:\n",
    "my_dataset = data.to_dict(orient = 'index')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importance Chart\n",
      "                      Feature P-value  Score\n",
      "0                      salary    0.85   1.92\n",
      "1                 to_messages    0.23   1.73\n",
      "2           deferral_payments    0.64   0.23\n",
      "3              total_payments    0.52   1.21\n",
      "4     exercised_stock_options    0.62   2.73\n",
      "5                       bonus    0.73   2.27\n",
      "6            restricted_stock    0.78   0.99\n",
      "7     shared_receipt_with_poi    0.01   8.20\n",
      "8   restricted_stock_deferred    0.89   0.05\n",
      "9           total_stock_value    0.66   2.61\n",
      "10                   expenses    0.79   0.65\n",
      "11              loan_advances    0.17   2.80\n",
      "12              from_messages    0.72   0.17\n",
      "13                      other    0.71   0.52\n",
      "14    from_this_person_to_poi    0.19   2.49\n",
      "15              director_fees    0.45   0.64\n",
      "16            deferred_income    0.62   1.41\n",
      "17        long_term_incentive    0.81   1.11\n",
      "18    from_poi_to_this_person    0.04   5.18\n",
      "19                    to_frac    0.10   3.16\n",
      "20                  from_frac    0.00  15.45\n",
      "21                total_money    0.60   2.00\n"
     ]
    }
   ],
   "source": [
    "### Investigate Variable Importance:\n",
    "\n",
    "select = SelectKBest(k = 'all')\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "labels = data['poi']\n",
    "features = data.drop('poi',axis = 1)\n",
    "\n",
    "# SelectKBest with multiple resamplings:\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(labels, 1500, random_state = 42)\n",
    "score = []\n",
    "pval = []\n",
    "for train_idx, test_idx in cv: \n",
    "    features_train = features.iloc[train_idx,:]\n",
    "    labels_train = labels[train_idx]\n",
    "    select.fit(features_train,labels_train)\n",
    "    score.append(select.scores_)\n",
    "    pval.append(select.pvalues_)\n",
    "    \n",
    "avgscore = np.mean(score, axis = 0)\n",
    "avgpval = np.mean(pval, axis = 0)\n",
    "\n",
    "\n",
    "print \"Variable Importance Chart\"\n",
    "print pd.DataFrame(\n",
    "    {'Feature': features.columns,\n",
    "     'Score': [ '%.2f' % elem for elem in avgscore],\n",
    "     'P-value': [ '%.2f' % elem for elem in avgpval]\n",
    "    })\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried including many features as computation cost is not a problem for this dataset. However, to my surprise, adding features makes the algorithms perform poorer. You will see below that I include all the features that has p-values < 0.3 in set feature_list1 and the same algorithm all perform poorer with this set. A possible explanation is that this dataset has many missing values (which we have filled them with 0 and this is wrong information).\n",
    "\n",
    "\n",
    "# Model building and Evaluation\n",
    "\n",
    "*Question 3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?*\n",
    "\n",
    "The algorithm I employed are a pipeline whose first step is PCA with 1 principle component and the last step is logistic regression. Since this is a small dataset that doesn’t take long to train, I have tried pipeline with PCA with other classifiers: decision tree, k nearest neighbor, random forest. I chose the model of PCA with logistic regression since it has the highest recall and above-30% precision. I would argue that in this problem, we care more about recall than precision. Random forest and decision tree perform decently though inferior to logistic regression. In one model, random forest outperforms logistic regression slightly in precision score but still have a lower recall. K nearest neighbors, to my surprise as I expect 'pois' would have similar behaviors, did not perform very well.\n",
    "\n",
    "\n",
    "\n",
    "** Model Performance Summary\n",
    "<pre>\n",
    "```\n",
    "\n",
    "| Model         | Features       | Precision | Recall |\n",
    "|---------------|----------------|-----------|--------|\n",
    "| K Neighbors   | feature_list2  |  0.24     |  0.14  |\n",
    "| Logistic Reg  | feature_list2  |  0.32     |  0.89  |** Best performance\n",
    "| Decision Tree | feature_list2  |  0.31     |  0.65  |\n",
    "| Random Forest | feature_list2  |  0.34     |  0.74  |\n",
    "| K Neighbors   | feature_list1  |  0.28     |  0.24  |\n",
    "| Logistic Reg  | feature_list1  |  0.22     |  0.87  |\n",
    "| Decision Tree | feature_list1  |  0.18     |  0.82  |\n",
    "| Random Forest | feature_list1  |  0.26     |  0.75  |\n",
    "| Logistic Reg  | feature_list21 |  0.27     |  0.77  | *effect of removing 'total_money' from best-performing model\n",
    "| Logistic Reg  | feature_list21 |  0.30     |  0.89  | *effect of removing 'to_frac' from best-performing model\n",
    "| Logistic Reg  | feature_list21 |  0.36     |  0.78  | *effect of removing 'from_frac' from best-performing model\n",
    "\n",
    "```\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=1, whiten=False)), ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('kn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='distance'))])\n",
      "\tAccuracy: 0.77364\tPrecision: 0.29885\tRecall: 0.18200\tF1: 0.22623\tF2: 0.19744\n",
      "\tTotal predictions: 11000\tTrue positives:  364\tFalse positives:  854\tFalse negatives: 1636\tTrue negatives: 8146\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=2, whiten=False)), ('clf', LogisticRegression(C=0.0001, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=111,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "\tAccuracy: 0.67118\tPrecision: 0.34132\tRecall: 0.86950\tF1: 0.49020\tF2: 0.66399\n",
      "\tTotal predictions: 11000\tTrue positives: 1739\tFalse positives: 3356\tFalse negatives:  261\tTrue negatives: 5644\n",
      "\n",
      "Pipeline(steps=[('pca', PCA(copy=True, n_components=2, whiten=False)), ('clf', DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=111, splitter='best'))])\n",
      "\tAccuracy: 0.67255\tPrecision: 0.32685\tRecall: 0.75600\tF1: 0.45638\tF2: 0.59876\n",
      "\tTotal predictions: 11000\tTrue positives: 1512\tFalse positives: 3114\tFalse negatives:  488\tTrue negatives: 5886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sets of features to train on:\n",
    "feature_list1 = ['to_messages','shared_receipt_with_poi','loan_advances','from_this_person_to_poi',\n",
    "             'from_poi_to_this_person','to_frac','from_frac'] # features with p-val < 0.3\n",
    "feature_list2 = ['from_frac','shared_receipt_with_poi','bonus']\n",
    "feature_list21 = feature_list2 + ['total_money'] # adding 'total_money'\n",
    "feature_list22 = feature_list2 + ['to_frac'] # adding 'to_frac'\n",
    "feature_list23 = ['shared_receipt_with_poi','exercised_stock_options'] # removing 'from_frac'\n",
    "\n",
    "\n",
    "feature_use = feature_list2 #Replace feature set to run on\n",
    "\n",
    "features = data[feature_use]\n",
    "features_list = ['poi'] + feature_use\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_kn = KNeighborsClassifier()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dectree = DecisionTreeClassifier(class_weight = 'balanced',random_state = 111)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_log = LogisticRegression(class_weight = 'balanced',random_state = 111)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(class_weight = 'balanced',random_state = 111)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "pca = PCA()\n",
    "n_compnt = range(len(feature_use)) + [3]\n",
    "n_compnt.remove(0)\n",
    "    \n",
    "    \n",
    "pipe_kn = Pipeline(steps = [('pca',pca),('scale',MinMaxScaler()),('kn',clf_kn)])\n",
    "param_grid_kn = dict(pca__n_components = n_compnt,\n",
    "                     kn__n_neighbors = [3,5,7,11],\n",
    "                     kn__weights = ['uniform','distance'])\n",
    "grid_search_kn = GridSearchCV(pipe_kn, param_grid_kn, scoring = 'recall')\n",
    "grid_search_kn.fit(features,labels)\n",
    "test_classifier(grid_search_kn.best_estimator_,my_dataset,features_list)\n",
    "\n",
    "\n",
    "pipe_log = Pipeline(steps = [('pca',pca),('clf',clf_log)])\n",
    "param_grid_log = dict(pca__n_components = n_compnt,\n",
    "                      clf__C = [0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,3])\n",
    "grid_search_log = GridSearchCV(pipe_log,param_grid_log,scoring = 'recall')\n",
    "grid_search_log.fit(features,labels)\n",
    "test_classifier(grid_search_log.best_estimator_,my_dataset,features_list)\n",
    "\n",
    "pipe_dectree = Pipeline(steps = [('pca',pca),('clf',clf_dectree)])\n",
    "param_grid_dectree = dict(pca__n_components = n_compnt,\n",
    "                          clf__max_depth = [1,2,3,5,7])\n",
    "grid_search_dectree = GridSearchCV(pipe_dectree,param_grid_dectree,scoring = 'recall')\n",
    "grid_search_dectree.fit(features,labels)\n",
    "test_classifier(grid_search_dectree.best_estimator_,my_dataset,features_list)\n",
    "\n",
    "pipe_rf = Pipeline(steps = [('pca',pca),('clf',clf_rf)])\n",
    "param_grid_rf = dict(pca__n_components = n_compnt,\n",
    "                     clf__max_depth = [1,2,3,5,7])\n",
    "grid_search_rf = GridSearchCV(pipe_rf,param_grid_rf,scoring = 'recall')\n",
    "grid_search_rf.fit(features,labels)\n",
    "test_classifier(grid_search_rf.best_estimator_,my_dataset,features_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question 4. What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? *\n",
    "\n",
    "Tuning parameters of an algorithm means selecting the set of parameters to optimize the performance of the algorithm. If tuning parameters isn’t done properly, the model may overfit or underfit.\n",
    "\n",
    "I used grid search with cross validation to tune the parameters. Since training the dataset takes place quickly, I tuned parameters for all-of-the-above algorithms (but only presented in the code the best three) and select the model that performs the best. I also tune the feature_format function, to see if the result is better if I replace missing values by medians instead of 0s. I find replacing the missing values by 0s yields the best result. \n",
    "\n",
    "\n",
    "*5.\tWhat is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis? *\n",
    "\n",
    "Validation is evaluating the performance of the model on new data. Without validation, it is very easy to choose an overfitting model that performs very well on known data but poorly on new data. I employ validation in my use of grid search with cross validation and my use of test_classifier of tester.py, which uses stratified shuffle split cross validator.\n",
    "\n",
    "\n",
    "# 4. Evaluation\n",
    "*6. Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. \n",
    "\n",
    "The precision and recall of my model is 32% and 898% respectively. This means that my algorithm predicts a ‘poi’ accurately 32% of the times; of all people predicted as a ‘poi’, 32% are actually ‘pois’. The recall suggests that my model is able to identify 89% of ‘pois’. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
